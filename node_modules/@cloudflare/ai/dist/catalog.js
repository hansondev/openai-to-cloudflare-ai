import { AiTextGeneration } from "./tasks/text-generation";
import { AiTextClassification } from "./tasks/text-classification";
import { AiTextEmbeddings } from "./tasks/text-embeddings";
import { AiTranslation } from "./tasks/translation";
import { AiSpeechRecognition } from "./tasks/speech-recognition";
import { AiImageClassification } from "./tasks/image-classification";
import { AiObjectDetection } from "./tasks/object-detection";
import { AiTextToImage } from "./tasks/text-to-image";
export const modelMappings = {
    "text-classification": {
        models: ["@cf/huggingface/distilbert-sst-2-int8"],
        class: AiTextClassification,
        id: "19606750-23ed-4371-aab2-c20349b53a60",
    },
    "text-to-image": {
        models: ["@cf/stabilityai/stable-diffusion-xl-base-1.0"],
        class: AiTextToImage,
        id: "3d6e1f35-341b-4915-a6c8-9a7142a9033a",
    },
    "text-embeddings": {
        models: ["@cf/baai/bge-small-en-v1.5", "@cf/baai/bge-base-en-v1.5", "@cf/baai/bge-large-en-v1.5"],
        class: AiTextEmbeddings,
        id: "0137cdcf-162a-4108-94f2-1ca59e8c65ee",
    },
    "speech-recognition": {
        models: ["@cf/openai/whisper", "@cf/openai/whisper-pipeline"],
        class: AiSpeechRecognition,
        id: "dfce1c48-2a81-462e-a7fd-de97ce985207",
    },
    "image-classification": {
        models: ["@cf/microsoft/resnet-50"],
        class: AiImageClassification,
        id: "00cd182b-bf30-4fc4-8481-84a3ab349657",
    },
    "object-detection": {
        models: ["@cf/meta/detr-resnet-50"],
        class: AiObjectDetection,
        id: "9c178979-90d9-49d8-9e2c-0f1cf01815d4",
    },
    "text-generation": {
        models: [
            "@cf/meta/llama-2-7b-chat-int8",
            "@cf/mistral/mistral-7b-instruct-v0.1",
            "@cf/meta/llama-2-7b-chat-fp16",
            "@hf/thebloke/llama-2-13b-chat-awq",
            "@hf/thebloke/zephyr-7b-beta-awq",
            "@hf/thebloke/mistral-7b-instruct-v0.1-awq",
            "@hf/thebloke/codellama-7b-instruct-awq",
        ],
        class: AiTextGeneration,
        id: "c329a1f9-323d-4e91-b2aa-582dd4188d34",
    },
    translation: {
        models: ["@cf/meta/m2m100-1.2b"],
        class: AiTranslation,
        id: "f57d07cb-9087-487a-bbbf-bc3e17fecc4b",
    },
};
export const modelSettings = {
    "@cf/meta/detr-resnet-50": {
        experimental: true,
    },
    "@cf/stabilityai/stable-diffusion-xl-base-1.0": {
        route: "stable-diffusion-xl-base-1-0",
    },
    "@hf/thebloke/llama-2-13b-chat-awq": {
        route: "thebloke-llama-2-13b-chat-awq",
        experimental: true,
        inputsDefaultsStream: {
            max_tokens: 596,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "llama2",
        },
        postProcessingFunc: (response) => {
            return response["generated_text"].value[0];
        },
        postProcessingFuncStream: (response) => {
            return response["generated_text"].value[0];
        },
    },
    "@hf/thebloke/zephyr-7b-beta-awq": {
        route: "thebloke-zephyr-7b-beta-awq",
        experimental: true,
        inputsDefaultsStream: {
            max_tokens: 596,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "zephyr",
        },
        postProcessingFunc: (response) => {
            return response["generated_text"].value[0];
        },
        postProcessingFuncStream: (response) => {
            return response["generated_text"].value[0];
        },
    },
    "@hf/thebloke/mistral-7b-instruct-v0.1-awq": {
        route: "thebloke-mistral-7b-instruct-v0-1-awq",
        experimental: true,
        inputsDefaultsStream: {
            max_tokens: 596,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "mistral-instruct",
        },
        postProcessingFunc: (response) => {
            return response["generated_text"].value[0];
        },
        postProcessingFuncStream: (response) => {
            return response["generated_text"].value[0];
        },
    },
    "@hf/thebloke/codellama-7b-instruct-awq": {
        route: "thebloke-codellama-7b-instruct-awq",
        inputsDefaultsStream: {
            max_tokens: 596,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "codellama-instruct",
        },
        postProcessingFunc: (response) => {
            return response["generated_text"].value[0];
        },
        postProcessingFuncStream: (response) => {
            return response["generated_text"].value[0];
        },
    },
    "@cf/meta/llama-2-7b-chat-fp16": {
        route: "llama-2-7b-chat-fp16",
        inputsDefaultsStream: {
            max_tokens: 2500,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "llama2",
        },
    },
    "@cf/meta/llama-2-7b-chat-int8": {
        route: "llama_2_7b_chat_int8",
        inputsDefaultsStream: {
            max_tokens: 1800,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "llama2",
        },
    },
    "@cf/openai/whisper-pipeline": {
        experimental: true,
        postProcessingFunc: (response) => {
            return {
                text: response["name"].value.join("").trim(),
                word_count: parseInt(response["word_count"].value),
                words: response["name"].value.map((w, i) => {
                    return {
                        word: w.trim(),
                        start: response["timestamps"].value[0][i][0],
                        end: response["timestamps"].value[0][i][1],
                    };
                }),
            };
        },
    },
    "@cf/mistral/mistral-7b-instruct-v0.1": {
        route: "mistral-7b-instruct-v0-1",
        inputsDefaultsStream: {
            max_tokens: 1800,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "mistral-instruct",
        },
    },
};
export const addModel = (task, model, settings) => {
    modelMappings[task].models.push(model);
    modelSettings[model] = settings;
};
